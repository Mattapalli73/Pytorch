{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUQzyl2tjMvl4MOUtinOij",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mattapalli73/Pytorch/blob/main/Automatic%20Differentiation%20with%20torch.autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import dataloader\n",
        "\n",
        "input=torch.ones(5)\n",
        "print(input)\n",
        "\n",
        "output=torch.zeros(3)\n",
        "print(output)\n",
        "\n",
        "par_w=torch.randn(5,3,requires_grad=True)\n",
        "print(par_w)\n",
        "\n",
        "par_b=torch.randn(3,requires_grad=True)\n",
        "print(par_b)\n",
        "\n",
        "output_z=torch.matmul(input,par_w)+par_b\n",
        "print(output_z)\n",
        "\n",
        "Loss=torch.nn.functional.binary_cross_entropy_with_logits(output_z,output)\n",
        "\n",
        "print('Gradient Function for output_z:',output_z.grad_fn)\n",
        "print('Gradient Function for Loss:',Loss.grad_fn)\n",
        "\n",
        "Loss.backward(retain_graph=True)\n",
        "print('the Gradient of par_w is :',par_w.grad)\n",
        "print('the Gradient of par_b is :',par_b.grad)\n",
        "Loss.backward(retain_graph=True)\n",
        "print('the Gradient of par_w is :',par_w.grad)\n",
        "print('the Gradient of par_b is :',par_b.grad)\n",
        "Loss.backward(retain_graph=True)\n",
        "print('the Gradient of par_w is :',par_w.grad)\n",
        "print('the Gradient of par_b is :',par_b.grad)\n",
        "\n",
        "print('Diabling the Gradint Function')\n",
        "\n",
        "output_z=torch.matmul(input,par_w)+par_b\n",
        "print(output_z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "  output_z=torch.matmul(input,par_w)+par_b\n",
        "print(output_z.requires_grad)\n",
        "\n",
        "print('Another way of Diabling the Gradint Function')\n",
        "output_z=torch.matmul(input,par_w)+par_b\n",
        "print(output_z.requires_grad)\n",
        "z_detach=output_z.detach()\n",
        "print(z_detach.requires_grad)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr_r2OrEJMKd",
        "outputId": "6cfacce2-3448-483f-a8cd-ca7c17e2fab5"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "tensor([0., 0., 0.])\n",
            "tensor([[ 0.9029, -0.1989,  0.6929],\n",
            "        [-1.1950,  0.7737,  0.4192],\n",
            "        [ 2.2198, -0.5059, -0.8130],\n",
            "        [ 0.7687,  0.5505,  0.2762],\n",
            "        [ 0.4306,  0.2674,  0.9799]], requires_grad=True)\n",
            "tensor([-1.3544, -1.2380,  1.3469], requires_grad=True)\n",
            "tensor([ 1.7724, -0.3512,  2.9021], grad_fn=<AddBackward0>)\n",
            "Gradient Function for output_z: <AddBackward0 object at 0x7fc1cc205110>\n",
            "Gradient Function for Loss: <BinaryCrossEntropyWithLogitsBackward0 object at 0x7fc1ccc733d0>\n",
            "the Gradient of par_w is : tensor([[0.2849, 0.1377, 0.3160],\n",
            "        [0.2849, 0.1377, 0.3160],\n",
            "        [0.2849, 0.1377, 0.3160],\n",
            "        [0.2849, 0.1377, 0.3160],\n",
            "        [0.2849, 0.1377, 0.3160]])\n",
            "the Gradient of par_b is : tensor([0.2849, 0.1377, 0.3160])\n",
            "the Gradient of par_w is : tensor([[0.5698, 0.2754, 0.6320],\n",
            "        [0.5698, 0.2754, 0.6320],\n",
            "        [0.5698, 0.2754, 0.6320],\n",
            "        [0.5698, 0.2754, 0.6320],\n",
            "        [0.5698, 0.2754, 0.6320]])\n",
            "the Gradient of par_b is : tensor([0.5698, 0.2754, 0.6320])\n",
            "the Gradient of par_w is : tensor([[0.8548, 0.4131, 0.9479],\n",
            "        [0.8548, 0.4131, 0.9479],\n",
            "        [0.8548, 0.4131, 0.9479],\n",
            "        [0.8548, 0.4131, 0.9479],\n",
            "        [0.8548, 0.4131, 0.9479]])\n",
            "the Gradient of par_b is : tensor([0.8548, 0.4131, 0.9479])\n",
            "Diabling the Gradint Function\n",
            "True\n",
            "False\n",
            "Another way of Diabling the Gradint Function\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    }
  ]
}