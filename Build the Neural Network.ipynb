{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPF0/Hx5oChxyFyFdMTCnpi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mattapalli73/Pytorch/blob/main/Build%20the%20Neural%20Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.container import Sequential\n",
        "from torch.nn.modules.activation import ReLU\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets,transforms\n",
        "Active_device='cude' if torch.cuda.is_available() else 'cpu'\n",
        "print('The currently used device is:',Active_device)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork,self).__init__()\n",
        "    self.flatten=nn.Flatten()\n",
        "    self.linear_relu_stack=nn.Sequential(\n",
        "    nn.Linear(28*28,512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 10),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x=self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "model=NeuralNetwork().to(Active_device)\n",
        "print('The Model is:',model)\n",
        "\n",
        "X=torch.rand(1, 28, 28, device=Active_device)\n",
        "logits=model(X)\n",
        "print('The logit is',logits)\n",
        "pred_probab=nn.Softmax(dim=1)(logits)\n",
        "y_pred=pred_probab.argmax(1)\n",
        "print(f\"prerdicted class is :{y_pred}\")\n",
        "\n",
        "input_img=torch.rand(3,28,28)\n",
        "print(input_img.size())\n",
        "\n",
        "print('The result for Flatten is')\n",
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_img)\n",
        "\n",
        "print(flat_image.size())\n",
        "\n",
        "print('Applying the ReLU')\n",
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())\n",
        "print(hidden1)\n",
        "#print('Applying the ReLU')\n",
        "#hidden2=nn.ReLU()(hidden1)\n",
        "#print(hidden2)\n",
        "#print(hidden2.size())\n",
        "\n",
        "#hidden3=nn.ReLU()(hidden2)\n",
        "#print(hidden3)\n",
        "#print(hidden3.size())\n",
        "\n",
        "print('@@@@@@@@@@@')\n",
        "\n",
        "print('The sequential Module is ')\n",
        "\n",
        "Seq_module=nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.Linear(20,10),\n",
        "    nn.ReLU(),\n",
        ")\n",
        "\n",
        "input_img=torch.rand(3,28,28)\n",
        "print('the input_img is',input_img)\n",
        "print(input_img.size())\n",
        "logits = Seq_module(input_img)\n",
        "print('the logit is',logits)\n",
        "print(logits.size())\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "print('the Predicted probability is',pred_probab)\n",
        "\n",
        "print('Model Parameters')\n",
        "\n",
        "print(\"Model structure: \", model, \"\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKIeoS_WEn7m",
        "outputId": "c34078ea-ce35-47a4-faf5-d644ca4247b2"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The currently used device is: cpu\n",
            "The Model is: NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "The logit is tensor([[-0.1191,  0.0255, -0.0526, -0.0375, -0.0074, -0.0927, -0.0766, -0.0935,\n",
            "         -0.0532,  0.0345]], grad_fn=<AddmmBackward0>)\n",
            "prerdicted class is :tensor([9])\n",
            "torch.Size([3, 28, 28])\n",
            "The result for Flatten is\n",
            "torch.Size([3, 784])\n",
            "Applying the ReLU\n",
            "torch.Size([3, 20])\n",
            "tensor([[ 0.0576,  0.3106,  0.2093,  0.2090, -0.2010, -0.2058,  0.0591, -0.4001,\n",
            "          0.3559,  0.2633,  0.1860, -0.1549,  0.3624,  0.0700,  0.0478, -0.0519,\n",
            "          0.1530,  0.2831, -0.7915,  0.2533],\n",
            "        [ 0.1664,  0.0858, -0.0193, -0.0122, -0.2322,  0.2055,  0.1341, -0.1424,\n",
            "          0.5823,  0.5497,  0.0982, -0.2112, -0.1181, -0.1006,  0.1970, -0.1151,\n",
            "          0.0169,  0.2115, -0.5044, -0.1582],\n",
            "        [-0.1173,  0.1171,  0.1199,  0.0973, -0.3948, -0.0396,  0.2628, -0.4354,\n",
            "          0.4757,  0.6511,  0.0385, -0.1960,  0.1181, -0.0785,  0.0703,  0.0596,\n",
            "          0.1386,  0.1421, -0.2863,  0.2824]], grad_fn=<AddmmBackward0>)\n",
            "@@@@@@@@@@@\n",
            "The sequential Module is \n",
            "the input_img is tensor([[[0.7298, 0.7152, 0.4524,  ..., 0.1425, 0.4887, 0.3971],\n",
            "         [0.8914, 0.6761, 0.9223,  ..., 0.8614, 0.4393, 0.2927],\n",
            "         [0.9197, 0.6814, 0.6151,  ..., 0.9992, 0.4323, 0.4023],\n",
            "         ...,\n",
            "         [0.1920, 0.9897, 0.9942,  ..., 0.2902, 0.7704, 0.3494],\n",
            "         [0.9425, 0.6701, 0.9972,  ..., 0.3230, 0.8616, 0.8606],\n",
            "         [0.2469, 0.5423, 0.8642,  ..., 0.4006, 0.9771, 0.5060]],\n",
            "\n",
            "        [[0.4708, 0.6626, 0.7923,  ..., 0.7870, 0.5277, 0.5923],\n",
            "         [0.6658, 0.3419, 0.7164,  ..., 0.3446, 0.2436, 0.3106],\n",
            "         [0.7987, 0.3783, 0.3867,  ..., 0.5367, 0.7243, 0.6253],\n",
            "         ...,\n",
            "         [0.8758, 0.3006, 0.8604,  ..., 0.2956, 0.2067, 0.3575],\n",
            "         [0.8038, 0.2516, 0.3602,  ..., 0.2487, 0.8272, 0.2048],\n",
            "         [0.9768, 0.4107, 0.8198,  ..., 0.0974, 0.7956, 0.1215]],\n",
            "\n",
            "        [[0.9658, 0.3101, 0.3724,  ..., 0.4534, 0.0749, 0.9431],\n",
            "         [0.8727, 0.9596, 0.4356,  ..., 0.7830, 0.4062, 0.2371],\n",
            "         [0.5891, 0.0749, 0.9390,  ..., 0.2315, 0.9910, 0.4742],\n",
            "         ...,\n",
            "         [0.4530, 0.8902, 0.6520,  ..., 0.0125, 0.6249, 0.0616],\n",
            "         [0.8305, 0.5808, 0.2672,  ..., 0.9972, 0.3168, 0.6453],\n",
            "         [0.6120, 0.6402, 0.6704,  ..., 0.2534, 0.2183, 0.6781]]])\n",
            "torch.Size([3, 28, 28])\n",
            "the logit is tensor([[0.3746, 0.2976, 0.0000, 0.0000, 0.0000, 0.0701, 0.0000, 0.0000, 0.0000,\n",
            "         0.1517],\n",
            "        [0.2315, 0.2728, 0.0000, 0.0000, 0.0000, 0.0101, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.3004, 0.4121, 0.0000, 0.0000, 0.0000, 0.1312, 0.0000, 0.0000, 0.0000,\n",
            "         0.0708]], grad_fn=<ReluBackward0>)\n",
            "torch.Size([3, 10])\n",
            "the Predicted probability is tensor([[0.1318, 0.1220, 0.0906, 0.0906, 0.0906, 0.0972, 0.0906, 0.0906, 0.0906,\n",
            "         0.1054],\n",
            "        [0.1191, 0.1241, 0.0945, 0.0945, 0.0945, 0.0954, 0.0945, 0.0945, 0.0945,\n",
            "         0.0945],\n",
            "        [0.1219, 0.1364, 0.0903, 0.0903, 0.0903, 0.1030, 0.0903, 0.0903, 0.0903,\n",
            "         0.0969]], grad_fn=<SoftmaxBackward0>)\n",
            "Model Parameters\n",
            "Model structure:  NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ") \n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0295, -0.0187,  0.0120,  ...,  0.0263,  0.0274,  0.0042],\n",
            "        [-0.0211, -0.0347, -0.0183,  ...,  0.0287,  0.0109, -0.0196]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0150,  0.0202], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0117, -0.0203, -0.0429,  ...,  0.0282,  0.0279,  0.0362],\n",
            "        [-0.0319, -0.0434, -0.0320,  ..., -0.0296, -0.0186, -0.0087]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0416, 0.0256], grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0117,  0.0414, -0.0372,  ...,  0.0390, -0.0084,  0.0115],\n",
            "        [ 0.0293, -0.0303,  0.0248,  ..., -0.0370, -0.0156,  0.0006]],\n",
            "       grad_fn=<SliceBackward0>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0127, -0.0102], grad_fn=<SliceBackward0>) \n",
            "\n"
          ]
        }
      ]
    }
  ]
}